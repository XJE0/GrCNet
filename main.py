import torch
from models import GrCNet, GrCNetConvOnly
from torch.autograd import Variable
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from copy import deepcopy
from preprocess import (  # ä¿®æ”¹å¯¼å…¥ï¼šä»åˆå¹¶çš„preprocess.pyå¯¼å…¥æ‰€æœ‰éœ€è¦çš„å‡½æ•°å’Œç±»
    read_entity_from_id, read_relation_from_id, 
    init_embeddings, build_data, Corpus, save_model
)

import random
import argparse
import os
import sys
import logging
import time
import pickle
import gc
import subprocess

# ğŸ¯ è®¾ç½®éšæœºç§å­
def set_seed(seed=42):
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # å¦‚æœä½¿ç”¨å¤šGPU
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    os.environ['PYTHONHASHSEED'] = str(seed)


# è®¾ç½®éšæœºç§å­
set_seed(42)
print("ğŸ¯ éšæœºç§å­å·²è®¾ç½®ä¸º 42")

CUDA_LAUNCH_BLOCKING = 1


def parse_args():
    args = argparse.ArgumentParser()
    # network arguments
    args.add_argument("-data", "--data",
                      default="./data/umls", help="data directory")
    args.add_argument("-e_g", "--epochs_gat", type=int,
                      default=200, help="Number of epochs")
    args.add_argument("-e_c", "--epochs_conv", type=int,
                      default=200, help="Number of epochs")
    args.add_argument("-w_gat", "--weight_decay_gat", type=float,
                      default=0.00001, help="L2 reglarization for gat")
    args.add_argument("-w_conv", "--weight_decay_conv", type=float,
                      default=1e-5, help="L2 reglarization for conv")
    args.add_argument("-pre_emb", "--pretrained_emb", type=bool,
                      default=True, help="Use pretrained embeddings")
    args.add_argument("-emb_size", "--embedding_size", type=int,
                      default=50, help="Size of embeddings (if pretrained not used)")
    args.add_argument("-l", "--lr", type=float, default=1.5e-3)
    args.add_argument("-outfolder", "--output_folder",
                      default="./checkpoints/umls/out/", help="Folder name to save the models.")

    # arguments for GAT
    args.add_argument("-b_gat", "--batch_size_gat", type=int,
                      default=128, help="Batch size for GAT")
    args.add_argument("-neg_s_gat", "--valid_invalid_ratio_gat", type=int,
                      default=2, help="Ratio of valid to invalid triples for GAT training")
    args.add_argument("-drop_GAT", "--drop_GAT", type=float,
                      default=0.3, help="Dropout probability for SpGAT layer")
    args.add_argument("-alpha", "--alpha", type=float,
                      default=0.2, help="LeakyRelu alphs for SpGAT layer")
    args.add_argument("-out_dim", "--entity_out_dim", type=int, nargs='+',
                      default=[100, 200], help="Entity output embedding dimensions")
    args.add_argument("-h_gat", "--nheads_GAT", type=int, nargs='+',
                      default=[2, 2], help="Multihead attention SpGAT")
    args.add_argument("-margin", "--margin", type=float,
                      default=1, help="Margin used in hinge loss")

    # arguments for convolution network
    args.add_argument("-b_conv", "--batch_size_conv", type=int,
                      default=128, help="Batch size for conv")
    args.add_argument("-alpha_conv", "--alpha_conv", type=float,
                      default=0.2, help="LeakyRelu alphas for conv layer")
    args.add_argument("-neg_s_conv", "--valid_invalid_ratio_conv", type=int, default=40,
                      help="Ratio of valid to invalid triples for convolution training")
    args.add_argument("-o", "--out_channels", type=int, default=50,
                      help="Number of output channels in conv layer")
    args.add_argument("-drop_conv", "--drop_conv", type=float,
                      default=0.3, help="Dropout probability for convolution layer")
    # ğŸ†• ä¿®æ”¹ï¼šé»˜è®¤å¯åŠ¨å¤šå¤´æ³¨æ„åŠ›ç­›é€‰ï¼Œä½†ä¼šæ ¹æ®å¹³å‡å…¥åº¦è‡ªåŠ¨è°ƒæ•´
    args.add_argument("-head_sel_ratio", "--head_selection_ratio", type=float,
                      default=1, help="Ratio of heads to keep during selection")

    # ğŸ†• æ–°å¢ï¼šéšæœºç§å­å‚æ•°
    args.add_argument("-seed", "--seed", type=int,
                      default=42, help="Random seed for reproducibility")

    # ğŸ†• æ–°å¢ï¼šç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•é€‰æ‹©å‚æ•°
    args.add_argument("-sim_method", "--similarity_method", type=str,
                      default="jaccard", choices=["jaccard", "cosine"],
                      help="Similarity calculation method: jaccard or cosine")
    args = args.parse_args()

    # ğŸ†• ä½¿ç”¨å‘½ä»¤è¡Œå‚æ•°ä¸­çš„ç§å­
    set_seed(args.seed)
    print(f"ğŸ¯ éšæœºç§å­å·²è®¾ç½®ä¸º {args.seed}")

    return args


args = parse_args()



def load_data(args):
    """
    ä¼˜åŒ–ç‰ˆæœ¬çš„æ•°æ®åŠ è½½å‡½æ•°ï¼Œæ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†ï¼Œè€ƒè™‘å‡ºè¾¹å’Œå…¥è¾¹çš„åˆ†å±‚æ¦‚å¿µç›¸ä¼¼æ€§
    """
    import time
    start_time = time.time()

    # 1. åŠ è½½åŸºç¡€æ•°æ®
    train_triples, train_data, validation_data, test_data, entity2id, relation2id, headTailSelector, unique_entities_train = build_data(
        args.data, is_unweigted=False, directed=True)

    num_entities = len(entity2id)
    num_relations = len(relation2id)
    print(f"æ•°æ®ç»Ÿè®¡: {num_entities} ä¸ªå®ä½“, {num_relations} ä¸ªå…³ç³»")

    # ğŸ¯ éªŒè¯æ•°æ®ç´¢å¼•èŒƒå›´
    max_entity_idx = max([max(triple[0], triple[2]) for triple in train_triples])
    max_relation_idx = max([triple[1] for triple in train_triples])

    print(f"è®­ç»ƒæ•°æ®ç´¢å¼•èŒƒå›´ - å®ä½“: 0-{max_entity_idx}, å…³ç³»: 0-{max_relation_idx}")
    print(f"æœ‰æ•ˆç´¢å¼•èŒƒå›´ - å®ä½“: 0-{num_entities - 1}, å…³ç³»: 0-{num_relations - 1}")

    if max_entity_idx >= num_entities:
        print(f"âŒ è­¦å‘Š: å®ä½“ç´¢å¼•è¶…å‡ºèŒƒå›´!")
    if max_relation_idx >= num_relations:
        print(f"âŒ è­¦å‘Š: å…³ç³»ç´¢å¼•è¶…å‡ºèŒƒå›´!")

    # ğŸ†• è®¡ç®—å¹³å‡å®ä½“å…¥åº¦
    print("ğŸ“Š è®¡ç®—å¹³å‡å®ä½“å…¥åº¦...")
    in_degree_counter = {}
    for triple in train_triples:
        tail_entity = triple[2]  # å°¾å®ä½“ç´¢å¼•
        in_degree_counter[tail_entity] = in_degree_counter.get(tail_entity, 0) + 1
    
    # è®¡ç®—å¹³å‡å…¥åº¦
    total_in_degree = sum(in_degree_counter.values())
    avg_in_degree = total_in_degree / num_entities if num_entities > 0 else 0
    
    print(f"ğŸ“Š å›¾è°±ç»Ÿè®¡ä¿¡æ¯:")
    print(f"   - æ€»å®ä½“æ•°: {num_entities}")
    print(f"   - æ€»å…¥åº¦æ•°: {total_in_degree}")
    print(f"   - å¹³å‡å®ä½“å…¥åº¦: {avg_in_degree:.2f}")
    
    # ğŸ†• æ ¹æ®å¹³å‡å…¥åº¦è‡ªåŠ¨å†³å®šæ˜¯å¦ä½¿ç”¨å¤´ç­›é€‰
    if avg_in_degree < 10:
        args.use_head_selection = False
        print(f"ğŸ“Š å¹³å‡å®ä½“å…¥åº¦ < 10ï¼Œè‡ªåŠ¨ç¦ç”¨å¤´ç­›é€‰åŠŸèƒ½")
    else:
        args.use_head_selection = True
        print(f"ğŸ“Š å¹³å‡å®ä½“å…¥åº¦ >= 10ï¼Œè‡ªåŠ¨å¯ç”¨å¤´ç­›é€‰åŠŸèƒ½")
    
    # æ‰“å°å…¥åº¦åˆ†å¸ƒä¿¡æ¯
    max_in_degree = max(in_degree_counter.values()) if in_degree_counter else 0
    min_in_degree = min(in_degree_counter.values()) if in_degree_counter else 0
    print(f"   - æœ€å¤§å®ä½“å…¥åº¦: {max_in_degree}")
    print(f"   - æœ€å°å®ä½“å…¥åº¦: {min_in_degree}")
    
    # è®¡ç®—å…¥åº¦ä¸º0çš„å®ä½“æ¯”ä¾‹
    zero_in_degree_entities = num_entities - len(in_degree_counter)
    zero_in_degree_ratio = zero_in_degree_entities / num_entities if num_entities > 0 else 0
    print(f"   - å…¥åº¦ä¸º0çš„å®ä½“æ¯”ä¾‹: {zero_in_degree_ratio:.2%}")

    # 2. åŠ è½½åµŒå…¥
    if args.pretrained_emb:
        entity_embeddings, relation_embeddings = init_embeddings(
            os.path.join(args.data, 'entity2vec.txt'),
            os.path.join(args.data, 'relation2vec.txt'))
        print("ä» TransE åˆå§‹åŒ–å…³ç³»å’Œå®ä½“åµŒå…¥")
    else:
        # ä½¿ç”¨æ›´é«˜æ•ˆçš„éšæœºåˆå§‹åŒ–
        entity_embeddings = np.random.randn(num_entities, args.embeding_size).astype(np.float32)
        relation_embeddings = np.random.randn(num_relations, args.embedding_size).astype(np.float32)
        print("éšæœºåˆå§‹åŒ–å…³ç³»å’Œå®ä½“åµŒå…¥")

    emb_time = time.time()
    print(f"åµŒå…¥åŠ è½½è€—æ—¶: {emb_time - start_time:.2f}ç§’")

    # 3. æ„å»ºåˆ†å±‚å¸ƒå°”çŸ©é˜µï¼ˆè€ƒè™‘å‡ºè¾¹å’Œå…¥è¾¹ï¼‰
    matrix_file_path = os.path.join(args.data, 'entity_2*relation_boolean_matrix.npy')

    if not os.path.exists(matrix_file_path):
        print("æ„å»ºå®ä½“-å…³ç³»åˆ†å±‚å¸ƒå°”çŸ©é˜µ...")
        
        # å½¢å¼èƒŒæ™¯çš„ç»´åº¦ï¼šå®ä½“æ•° Ã— (2 Ã— å…³ç³»æ•°)
        # å‰num_relationsåˆ—å¯¹åº”r_outï¼Œånum_relationsåˆ—å¯¹åº”r_in
        boolean_matrix = np.zeros((num_entities, 2 * num_relations), dtype=np.bool_)
        
        # å¤„ç†æ¯ä¸ªè®­ç»ƒä¸‰å…ƒç»„
        for triple in train_triples:
            h_idx, r_idx, t_idx = triple
            
            # å¤´å®ä½“å…·æœ‰r_outå±æ€§
            boolean_matrix[h_idx, r_idx] = True  # r_outåœ¨çŸ©é˜µçš„å‰åŠéƒ¨åˆ†
            
            # å°¾å®ä½“å…·æœ‰r_inå±æ€§  
            boolean_matrix[t_idx, r_idx + num_relations] = True  # r_inåœ¨çŸ©é˜µçš„ååŠéƒ¨åˆ†
        
        np.save(matrix_file_path, boolean_matrix.astype(np.int32))
        print(f"ä¿å­˜å®ä½“-å…³ç³»å¸ƒå°”çŸ©é˜µåˆ° {matrix_file_path}")
        print(f"å¸ƒå°”çŸ©é˜µå½¢çŠ¶: {boolean_matrix.shape} (å®ä½“æ•°: {num_entities}, å±æ€§æ•°: {2 * num_relations})")
    else:
        print(f"åŠ è½½ç°æœ‰å¸ƒå°”çŸ©é˜µ: {matrix_file_path}")
        boolean_matrix = np.load(matrix_file_path)
        print(f"å¸ƒå°”çŸ©é˜µå½¢çŠ¶: {boolean_matrix.shape}")

    bool_matrix_time = time.time()
    print(f"å¸ƒå°”çŸ©é˜µå¤„ç†è€—æ—¶: {bool_matrix_time - emb_time:.2f}ç§’")

    # 4. è®¡ç®—åŒå‘ç›¸ä¼¼åº¦çŸ©é˜µå¹¶èåˆ
    A_bool = boolean_matrix.astype(np.float32)  # è½¬ä¸ºæµ®ç‚¹æ–¹ä¾¿è®¡ç®—

    # 4.1 åˆ†åˆ«è®¡ç®—å‡ºè¾¹å’Œå…¥è¾¹çš„ç›¸ä¼¼åº¦
    similarity_method = getattr(args, 'similarity_method', 'jaccard')  # é»˜è®¤ä¸ºJaccard
    
    # è®¡ç®—æ¯ä¸ªå®ä½“åœ¨å‡ºè¾¹å’Œå…¥è¾¹ä¸Šçš„å…³ç³»æ•°
    row_sum_out = A_bool[:, :num_relations].sum(axis=1)  # å‡ºè¾¹å…³ç³»æ•°
    row_sum_in = A_bool[:, num_relations:].sum(axis=1)   # å…¥è¾¹å…³ç³»æ•°
    row_sum_total = row_sum_out + row_sum_in              # æ€»å…³ç³»æ•°

    # å¯¹äºå¤§è§„æ¨¡æ•°æ®é›†ï¼Œç›´æ¥è®¡ç®—åˆ†å±‚QçŸ©é˜µè€Œä¸å­˜å‚¨å®Œæ•´çš„SçŸ©é˜µ
    Q_file = os.path.join(args.data, f'granular_weight_Q_{similarity_method}_layered.npy')

    if not os.path.exists(Q_file):
        print(f"ä½¿ç”¨åˆ†å±‚æ¦‚å¿µç›¸ä¼¼æ€§è®¡ç®—ç²’åº¦æƒé‡çŸ©é˜µ Q ({similarity_method}ç›¸ä¼¼åº¦)...")

        # æ£€æŸ¥æ•°æ®é›†è§„æ¨¡ï¼Œå†³å®šæ˜¯å¦ä½¿ç”¨åˆ†å—è®¡ç®—
        if num_entities > 10000:  # å¤§è§„æ¨¡æ•°æ®é›†
            print(f"æ£€æµ‹åˆ°å¤§è§„æ¨¡æ•°æ®é›† ({num_entities} ä¸ªå®ä½“)ï¼Œä½¿ç”¨åˆ†å—è®¡ç®—...")
            Q = _compute_Q_layered_large_scale(A_bool, row_sum_out, row_sum_in, row_sum_total, 
                                             similarity_method, num_entities, num_relations)
        else:  # å°è§„æ¨¡æ•°æ®é›†
            print("ä½¿ç”¨å¸¸è§„è®¡ç®—...")
            Q = _compute_Q_layered_normal(A_bool, row_sum_out, row_sum_in, row_sum_total,
                                        similarity_method, num_entities, num_relations)

        np.save(Q_file, Q)
        print(f"ä¿å­˜åˆ†å±‚ç²’åº¦æƒé‡çŸ©é˜µ Q åˆ° {Q_file}")

        # æ‰“å°QçŸ©é˜µçš„ç»Ÿè®¡ä¿¡æ¯
        print(f"åˆ†å±‚æƒé‡çŸ©é˜µQç»Ÿè®¡: æœ€å°å€¼={Q.min():.4f}, æœ€å¤§å€¼={Q.max():.4f}, å‡å€¼={Q.mean():.4f}")
        print(f"æƒé‡çŸ©é˜µQå½¢çŠ¶: {Q.shape}")

    else:
        print(f"åŠ è½½ç°æœ‰åˆ†å±‚ç²’åº¦æƒé‡çŸ©é˜µ: {Q_file}")
        Q = np.load(Q_file)

    Q_time = time.time()
    print(f"æƒé‡çŸ©é˜µè®¡ç®—è€—æ—¶: {Q_time - bool_matrix_time:.2f}ç§’")

    print(f"æœ€ç»ˆæƒé‡çŸ©é˜µå½¢çŠ¶: {Q.shape} (å®ä½“Ã—å…³ç³»)")

    # 5. åˆ›å»ºè¯­æ–™åº“
    print("åˆ›å»ºæ•°æ®è¯­æ–™åº“...")
    corpus = Corpus(
        args, train_data, validation_data, test_data, entity2id, relation2id,
        headTailSelector, args.batch_size_gat, args.valid_invalid_ratio_gat,
        unique_entities_train, granularity_matrix=Q
    )

    total_time = time.time() - start_time
    print(f"æ•°æ®åŠ è½½æ€»è€—æ—¶: {total_time:.2f}ç§’")

    # å°†åµŒå…¥è½¬ç§»åˆ°GPU
    entity_tensor = torch.FloatTensor(entity_embeddings)
    relation_tensor = torch.FloatTensor(relation_embeddings)

    if torch.cuda.is_available():
        entity_tensor = entity_tensor.cuda()
        relation_tensor = relation_tensor.cuda()

    return corpus, entity_tensor, relation_tensor


def _compute_layered_similarity_block(A_bool, row_sum_out, row_sum_in, row_sum_total, 
                                    similarity_method, entity_indices, full_entity_indices=None):
    """
    è®¡ç®—å®ä½“å—çš„åŒå‘ç›¸ä¼¼åº¦ï¼ˆå‡ºè¾¹ç›¸ä¼¼åº¦ + å…¥è¾¹ç›¸ä¼¼åº¦ï¼‰
    """
    if full_entity_indices is None:
        full_entity_indices = np.arange(A_bool.shape[0])
    
    num_relations = A_bool.shape[1] // 2
    
    # æå–å‡ºè¾¹å’Œå…¥è¾¹çš„å¸ƒå°”çŸ©é˜µ
    A_out = A_bool[:, :num_relations]
    A_in = A_bool[:, num_relations:]
    
    # æå–å½“å‰å—çš„å®ä½“å…³ç³»å‘é‡
    A_out_block = A_out[entity_indices, :]
    A_in_block = A_in[entity_indices, :]
    
    # è®¡ç®—å‡ºè¾¹ç›¸ä¼¼åº¦
    intersection_out = A_out_block @ A_out[full_entity_indices, :].T
    
    # è®¡ç®—å…¥è¾¹ç›¸ä¼¼åº¦  
    intersection_in = A_in_block @ A_in[full_entity_indices, :].T
    
    if similarity_method == 'jaccard':
        # å‡ºè¾¹Jaccardç›¸ä¼¼åº¦
        row_sum_out_block = row_sum_out[entity_indices]
        row_sum_out_full = row_sum_out[full_entity_indices]
        union_out = row_sum_out_block[:, np.newaxis] + row_sum_out_full - intersection_out
        union_out_safe = np.where(union_out == 0, 1.0, union_out)
        S_out = intersection_out / union_out_safe
        
        # å…¥è¾¹Jaccardç›¸ä¼¼åº¦
        row_sum_in_block = row_sum_in[entity_indices]
        row_sum_in_full = row_sum_in[full_entity_indices]
        union_in = row_sum_in_block[:, np.newaxis] + row_sum_in_full - intersection_in
        union_in_safe = np.where(union_in == 0, 1.0, union_in)
        S_in = intersection_in / union_in_safe
        
    elif similarity_method == 'cosine':
        # å‡ºè¾¹ä½™å¼¦ç›¸ä¼¼åº¦
        row_sum_out_block = row_sum_out[entity_indices]
        row_sum_out_full = row_sum_out[full_entity_indices]
        row_sum_out_block_safe = np.where(row_sum_out_block == 0, 1.0, row_sum_out_block)
        row_sum_out_full_safe = np.where(row_sum_out_full == 0, 1.0, row_sum_out_full)
        denom_out = np.sqrt(np.outer(row_sum_out_block_safe, row_sum_out_full_safe))
        S_out = intersection_out / denom_out
        
        # å…¥è¾¹ä½™å¼¦ç›¸ä¼¼åº¦
        row_sum_in_block = row_sum_in[entity_indices]
        row_sum_in_full = row_sum_in[full_entity_indices]
        row_sum_in_block_safe = np.where(row_sum_in_block == 0, 1.0, row_sum_in_block)
        row_sum_in_full_safe = np.where(row_sum_in_full == 0, 1.0, row_sum_in_full)
        denom_in = np.sqrt(np.outer(row_sum_in_block_safe, row_sum_in_full_safe))
        S_in = intersection_in / denom_in
        
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ç›¸ä¼¼åº¦è®¡ç®—æ–¹æ³•: {similarity_method}")
    
    # èåˆå‡ºè¾¹å’Œå…¥è¾¹ç›¸ä¼¼åº¦ï¼šå–å¹³å‡å€¼
    S_combined = (S_out + S_in) / 2.0
    
    # ç¡®ä¿æ— æ•ˆè¡Œä¸ºé›¶
    valid_rows_block = row_sum_total[entity_indices] > 0
    valid_rows_full = row_sum_total[full_entity_indices] > 0
    valid_mask = np.outer(valid_rows_block, valid_rows_full)
    S_combined[~valid_mask] = 0
    
    return S_combined


def _compute_Q_layered_normal(A_bool, row_sum_out, row_sum_in, row_sum_total, 
                            similarity_method, num_entities, num_relations):
    """
    å¸¸è§„è®¡ç®—åˆ†å±‚QçŸ©é˜µï¼ˆé€‚åˆå°è§„æ¨¡æ•°æ®é›†ï¼‰
    """
    print("ä½¿ç”¨åˆ†å±‚å¸¸è§„è®¡ç®—æ–¹å¼...")
    
    # è®¡ç®—å®Œæ•´çš„åˆ†å±‚ç›¸ä¼¼åº¦çŸ©é˜µ S
    S = _compute_layered_similarity_block(A_bool, row_sum_out, row_sum_in, row_sum_total,
                                        similarity_method, np.arange(num_entities), np.arange(num_entities))
    
    # æ³¨æ„ï¼šA_boolç°åœ¨æ˜¯æ‰©å±•çš„å¸ƒå°”çŸ©é˜µï¼Œå½¢çŠ¶ä¸º (num_entities, 2*num_relations)
    # ä½†QçŸ©é˜µåº”è¯¥ä¿æŒå½¢çŠ¶ä¸º (num_entities, num_relations)ï¼Œå› ä¸ºåç»­æ¨¡å‹æœŸæœ›çš„æ˜¯æ¯ä¸ªå…³ç³»çš„æƒé‡
    
    # åˆ†åˆ«è®¡ç®—å‡ºè¾¹å’Œå…¥è¾¹çš„è´¡çŒ®ï¼Œç„¶ååˆå¹¶
    A_out = A_bool[:, :num_relations]  # å‡ºè¾¹éƒ¨åˆ†
    A_in = A_bool[:, num_relations:]   # å…¥è¾¹éƒ¨åˆ†
    
    # è®¡ç®—å‡ºè¾¹æƒé‡
    numerator_out = S @ A_out
    # è®¡ç®—å…¥è¾¹æƒé‡  
    numerator_in = S @ A_in
    
    # åˆå¹¶å‡ºè¾¹å’Œå…¥è¾¹çš„æƒé‡ï¼ˆå–å¹³å‡å€¼æˆ–å…¶ä»–èåˆç­–ç•¥ï¼‰
    numerator_combined = (numerator_out + numerator_in) / 2.0
    
    denominator = S.sum(axis=1)
    denominator_safe = np.where(denominator == 0, 1.0, denominator)
    
    # è®¡ç®—æœ€ç»ˆçš„QçŸ©é˜µ
    Q = numerator_combined / denominator_safe[:, np.newaxis]
    
    return Q


def _compute_Q_layered_large_scale(A_bool, row_sum_out, row_sum_in, row_sum_total,
                                 similarity_method, num_entities, num_relations, block_size=1000):
    """
    å¤§è§„æ¨¡æ•°æ®é›†çš„åˆ†å—è®¡ç®—åˆ†å±‚QçŸ©é˜µ
    """
    print(f"ä½¿ç”¨åˆ†å±‚åˆ†å—è®¡ç®—æ–¹å¼ï¼Œå—å¤§å°: {block_size}")
    
    # åˆå§‹åŒ–QçŸ©é˜µ
    Q = np.zeros((num_entities, num_relations), dtype=np.float32)
    
    # åˆ†å—è®¡ç®—
    num_blocks = (num_entities + block_size - 1) // block_size
    
    for block_idx in range(num_blocks):
        start_idx = block_idx * block_size
        end_idx = min((block_idx + 1) * block_size, num_entities)
        entity_indices = np.arange(start_idx, end_idx)
        
        print(f"å¤„ç†å— {block_idx + 1}/{num_blocks} (å®ä½“ {start_idx}-{end_idx - 1})")
        
        # è®¡ç®—å½“å‰å—ä¸æ‰€æœ‰å®ä½“çš„åˆ†å±‚ç›¸ä¼¼åº¦
        S_block = _compute_layered_similarity_block(A_bool, row_sum_out, row_sum_in, row_sum_total,
                                                  similarity_method, entity_indices, np.arange(num_entities))
        
        # åˆ†åˆ«å¤„ç†å‡ºè¾¹å’Œå…¥è¾¹
        A_out = A_bool[:, :num_relations]
        A_in = A_bool[:, num_relations:]
        
        numerator_out_block = S_block @ A_out
        numerator_in_block = S_block @ A_in
        numerator_combined_block = (numerator_out_block + numerator_in_block) / 2.0
        
        denominator_block = S_block.sum(axis=1)
        denominator_safe_block = np.where(denominator_block == 0, 1.0, denominator_block)
        
        Q_block = numerator_combined_block / denominator_safe_block[:, np.newaxis]
        Q[entity_indices] = Q_block
        
        # é‡Šæ”¾å†…å­˜
        del S_block, numerator_out_block, numerator_in_block, numerator_combined_block, Q_block
        
    return Q

Corpus_, entity_embeddings, relation_embeddings = load_data(args)

entity_embeddings_copied = deepcopy(entity_embeddings).cuda()
relation_embeddings_copied = deepcopy(relation_embeddings).cuda()

print("Initial entity dimensions {} , relation dimensions {}".format(
    entity_embeddings.size(), relation_embeddings.size()))

CUDA = torch.cuda.is_available()
print(CUDA)


def batch_gat_loss(gat_loss_func, train_indices, entity_embed, relation_embed):
    len_pos_triples = int(
        train_indices.shape[0] / (int(args.valid_invalid_ratio_gat) + 1))

    pos_triples = train_indices[:len_pos_triples]
    neg_triples = train_indices[len_pos_triples:]

    pos_triples = pos_triples.repeat(int(args.valid_invalid_ratio_gat), 1)

    source_embeds = entity_embed[pos_triples[:, 0]]
    relation_embeds = relation_embed[pos_triples[:, 1]]
    tail_embeds = entity_embed[pos_triples[:, 2]]

    x = source_embeds + relation_embeds - tail_embeds
    pos_norm = torch.norm(x, p=1, dim=1)

    source_embeds = entity_embed[neg_triples[:, 0]]
    relation_embeds = relation_embed[neg_triples[:, 1]]
    tail_embeds = entity_embed[neg_triples[:, 2]]

    x = source_embeds + relation_embeds - tail_embeds
    neg_norm = torch.norm(x, p=1, dim=1)

    y = -torch.ones(int(args.valid_invalid_ratio_gat) * len_pos_triples).cuda()

    loss = gat_loss_func(pos_norm, neg_norm, y)
    return loss


def train_gat(args):
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    if not os.path.exists(args.output_folder):
        os.makedirs(args.output_folder)

    # ğŸ†• åˆ›å»ºæ”¹è¿›çš„GATæ¨¡å‹ï¼ˆæ”¯æŒå¤šå¤´ç­›é€‰ï¼‰
    print("Defining model with head selection")
    print(
        f"Model type -> GAT layer with {args.nheads_GAT[0]} heads, Head selection: {args.use_head_selection}, Ratio: {args.head_selection_ratio}")

    model_gat = GrCNet(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
                                args.drop_GAT, args.alpha, args.nheads_GAT,
                                use_head_selection=args.use_head_selection,
                                head_selection_ratio=args.head_selection_ratio)
    if CUDA:
        model_gat.cuda()

    optimizer = torch.optim.Adam(
        model_gat.parameters(), lr=args.lr, weight_decay=args.weight_decay_gat)

    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=500, gamma=0.5, last_epoch=-1)

    gat_loss_func = nn.MarginRankingLoss(margin=args.margin)

    train_log_file = f"{args.output_folder}/gat_train_log.txt"
     
    print("Starting GAT training from scratch")
    with open(train_log_file, "w") as f:
        f.write("Iteration,Iteration_time,Iteration_loss,Epoch,Average_loss,Epoch_time\n")
    start_epoch = 0

    epoch_losses = []  # losses of all epochs
    print("Number of epochs {}".format(args.epochs_gat))

    for epoch in range(start_epoch, args.epochs_gat):
        print("\nepoch-> ", epoch)
        random.shuffle(Corpus_.train_triples)
        Corpus_.train_indices = np.array(
            list(Corpus_.train_triples)).astype(np.int32)

        model_gat.train()  # getting in training mode
        start_time = time.time()
        epoch_loss = []

        if len(Corpus_.train_indices) % args.batch_size_gat == 0:
            num_iters_per_epoch = len(
                Corpus_.train_indices) // args.batch_size_gat
        else:
            num_iters_per_epoch = (
                                          len(Corpus_.train_indices) // args.batch_size_gat) + 1

        for iters in range(num_iters_per_epoch):
            start_time_iter = time.time()
            train_indices, train_values = Corpus_.get_iteration_batch(iters)

            if CUDA:
                train_indices = Variable(
                    torch.LongTensor(train_indices)).cuda()
                train_values = Variable(torch.FloatTensor(train_values)).cuda()
            else:
                train_indices = Variable(torch.LongTensor(train_indices))
                train_values = Variable(torch.FloatTensor(train_values))

            # forward pass
            entity_embed, relation_embed, _ = model_gat(
                Corpus_, Corpus_.train_adj_matrix, train_indices)
            # ğŸ¯ æ€§èƒ½ç›‘æ§ï¼šæ·»åŠ åœ¨è¿™é‡Œ
            if args.use_head_selection and iters % 50 == 0:  # æ¯50ä¸ªè¿­ä»£ç›‘æ§ä¸€æ¬¡
                analysis_data = model_gat.get_attention_analysis_data()
                if analysis_data and 'computation_savings' in analysis_data:
                    savings = analysis_data['computation_savings']
                    print(f"ğŸ¯ å¤´é€‰æ‹©ç»Ÿè®¡: {savings['selected_heads']}/{savings['total_heads']} ä¸ªå¤´è¢«é€‰ä¸­ "
                          f"(è®¡ç®—é‡å‡å°‘ {savings['computation_reduced']:.1f}%)")
                # è®°å½•åˆ°æ—¥å¿—æ–‡ä»¶
                with open(train_log_file, 'a') as f:
                    if analysis_data and 'computation_savings' in analysis_data:
                        savings = analysis_data['computation_savings']
                        f.write(
                            f"Head Selection -> Iteration {iters}: {savings['selected_heads']}/{savings['total_heads']} heads, "
                            f"Reduction: {savings['computation_reduced']:.1f}%\n")

            optimizer.zero_grad()

            loss = batch_gat_loss(
                gat_loss_func, train_indices, entity_embed, relation_embed)

            loss.backward()
            optimizer.step()

            epoch_loss.append(loss.data.item())

            end_time_iter = time.time()

            print("Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}".format(
                iters, end_time_iter - start_time_iter, loss.data.item()))
            with open(train_log_file, 'a') as f:
                f.write("Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\n".format(
                    iters, end_time_iter - start_time_iter, loss.data.item()))

        scheduler.step()
        avg_loss = sum(epoch_loss) / len(epoch_loss)
        epoch_time = time.time() - start_time

        # ğŸ¯ æ€§èƒ½ç›‘æ§ï¼šæ¯ä¸ªepochç»“æŸæ—¶çš„æ€»ç»“
        if args.use_head_selection:
            analysis_data = model_gat.get_attention_analysis_data()
            if analysis_data and 'computation_savings' in analysis_data:
                savings = analysis_data['computation_savings']
                print(f"ğŸ“Š Epoch {epoch} å¤´é€‰æ‹©æ€»ç»“: {savings['selected_heads']}/{savings['total_heads']} ä¸ªå¤´è¢«é€‰ä¸­ "
                      f"(æ€»ä½“è®¡ç®—é‡å‡å°‘ {savings['computation_reduced']:.1f}%)")

                with open(train_log_file, 'a') as f:
                    f.write(
                        f"Epoch {epoch} Head Selection Summary: {savings['selected_heads']}/{savings['total_heads']} heads, "
                        f"Overall Reduction: {savings['computation_reduced']:.1f}%\n")

        print("Epoch {} , average loss {} , epoch_time {}".format(
            epoch, avg_loss, epoch_time))
        with open(train_log_file, 'a') as f:
            f.write("Epoch {} , average loss {} , epoch_time {}\n".format(
                epoch, avg_loss, epoch_time))
        epoch_losses.append(avg_loss)

        # ä¿å­˜æ¨¡å‹
    save_model(model_gat, args.data, epoch, args.output_folder)


def train_conv(args):
    # åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹
    conv_output_folder = f"{args.output_folder}/conv/"
    if not os.path.exists(conv_output_folder):
        os.makedirs(conv_output_folder)

    # ğŸ†• åˆ›å»ºæ¨¡å‹æ—¶ä½¿ç”¨ç›¸åŒçš„å¤´é€‰æ‹©å‚æ•°
    print("Defining model")
    model_gat = GrCNet(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
                                args.drop_GAT, args.alpha, args.nheads_GAT,
                                use_head_selection=args.use_head_selection,  # ğŸ†• ä¿æŒä¸€è‡´
                                head_selection_ratio=args.head_selection_ratio)  # ğŸ†• ä¿æŒä¸€è‡´

    print("Only Conv model trained")
    model_conv = GrCNetConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,
                                 args.nheads_GAT, args.out_channels)

    if CUDA:
        model_conv.cuda()
        model_gat.cuda()

    model_gat.load_state_dict(
        torch.load(f'{args.output_folder}/trained_{args.epochs_gat - 1}.pth'),
        strict=False
    )

    # ğŸ†• ç¡®ä¿åµŒå…¥å‚æ•°æ­£ç¡®ä¼ é€’
    model_conv.final_entity_embeddings.data = model_gat.final_entity_embeddings.data
    model_conv.final_relation_embeddings.data = model_gat.final_relation_embeddings.data

    Corpus_.batch_size = args.batch_size_conv
    Corpus_.invalid_valid_ratio = int(args.valid_invalid_ratio_conv)

    optimizer = torch.optim.Adam(
        model_conv.parameters(), lr=args.lr, weight_decay=args.weight_decay_conv)

    scheduler = torch.optim.lr_scheduler.StepLR(
        optimizer, step_size=25, gamma=0.5, last_epoch=-1)

    margin_loss = torch.nn.SoftMarginLoss()
    train_log_file = f"{conv_output_folder}/conv_train_log.txt"

    print("Starting Conv training from scratch")
    with open(train_log_file, "w") as f:
        f.write("Iteration,Iteration_time,Iteration_loss,Epoch,Average_loss,Epoch_time\n")
    start_epoch = 0

    epoch_losses = []  # losses of all epochs
    print("Number of epochs {}".format(args.epochs_conv))

    for epoch in range(start_epoch, args.epochs_conv):
        print("\nepoch-> ", epoch)
        random.shuffle(Corpus_.train_triples)
        Corpus_.train_indices = np.array(
            list(Corpus_.train_triples)).astype(np.int32)

        model_conv.train()  # getting in training mode
        start_time = time.time()
        epoch_loss = []

        if len(Corpus_.train_indices) % args.batch_size_conv == 0:
            num_iters_per_epoch = len(
                Corpus_.train_indices) // args.batch_size_conv
        else:
            num_iters_per_epoch = (
                                          len(Corpus_.train_indices) // args.batch_size_conv) + 1

        for iters in range(num_iters_per_epoch):
            start_time_iter = time.time()
            train_indices, train_values = Corpus_.get_iteration_batch(iters)

            if CUDA:
                train_indices = Variable(
                    torch.LongTensor(train_indices)).cuda()
                train_values = Variable(torch.FloatTensor(train_values)).cuda()
            else:
                train_indices = Variable(torch.LongTensor(train_indices))
                train_values = Variable(torch.FloatTensor(train_values))

            preds = model_conv(
                Corpus_, Corpus_.train_adj_matrix, train_indices)

            optimizer.zero_grad()

            loss = margin_loss(preds.view(-1), train_values.view(-1))

            loss.backward()
            optimizer.step()

            epoch_loss.append(loss.data.item())

            end_time_iter = time.time()

            print("Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}".format(
                iters, end_time_iter - start_time_iter, loss.data.item()))
            with open(train_log_file, 'a') as f:
                f.write("Iteration-> {0}  , Iteration_time-> {1:.4f} , Iteration_loss {2:.4f}\n".format(
                    iters, end_time_iter - start_time_iter, loss.data.item()))

        scheduler.step()
        avg_loss = sum(epoch_loss) / len(epoch_loss)
        epoch_time = time.time() - start_time
        print("Epoch {} , average loss {} , epoch_time {}".format(
            epoch, avg_loss, epoch_time))
        with open(train_log_file, 'a') as f:
            f.write("Epoch {} , average loss {} , epoch_time {}\n".format(
                epoch, avg_loss, epoch_time))
        epoch_losses.append(avg_loss)

        # ä¿å­˜æ¨¡å‹
    save_model(model_conv, args.data, epoch, conv_output_folder)


def evaluate_conv(args, unique_entities):
    model_conv = GrCNetConvOnly(entity_embeddings, relation_embeddings, args.entity_out_dim, args.entity_out_dim,
                                 args.drop_GAT, args.drop_conv, args.alpha, args.alpha_conv,
                                 args.nheads_GAT, args.out_channels)

    # ğŸ†• ä¿®å¤ï¼šä½¿ç”¨ strict=False åŠ è½½æ¨¡å‹
    conv_checkpoint_path = f'{args.output_folder}conv/trained_{args.epochs_conv - 1}.pth'
    if os.path.exists(conv_checkpoint_path):
        model_conv.load_state_dict(torch.load(conv_checkpoint_path), strict=False)
    else:
        print(f"è­¦å‘Š: æ‰¾ä¸åˆ°å·ç§¯æ¨¡å‹æ£€æŸ¥ç‚¹ {conv_checkpoint_path}")
        return

    model_conv.cuda()
    model_conv.eval()
    with torch.no_grad():
        Corpus_.get_validation_pred(args, model_conv, unique_entities)


# è‡ªåŠ¨æ£€æµ‹å¹¶æ¢å¤è®­ç»ƒ
print("è‡ªåŠ¨æ£€æµ‹è®­ç»ƒçŠ¶æ€...")
train_gat(args)
train_conv(args)
evaluate_conv(args, Corpus_.unique_entities_train)